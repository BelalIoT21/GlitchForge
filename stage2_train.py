"""
Stage 2 Main - ML Model Development
Orchestrates the complete ML pipeline:
1. Collect NVD data (15,000 CVEs)
2. Engineer features
3. Train Random Forest
4. Train Neural Network
5. Evaluate and compare models
"""

import logging
import pandas as pd
from pathlib import Path
import sys
import os
import warnings

# --- Environment Setup & Warning Suppression ---
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN messages

# Suppress Pandas FutureWarning noise from inplace operations
warnings.simplefilter(action='ignore', category=FutureWarning)

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.ml.nvd_collector import NVDCollector
from src.ml.feature_engineering import FeatureEngineer
from src.ml.model_trainer import MLModelTrainer

def setup_logging():
    """Setup logging configuration"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s', # Simplified format
        handlers=[
            logging.FileHandler('stage2_training.log'),
            logging.StreamHandler()
        ]
    )

def collect_nvd_data():
    """Collect NVD data"""
    try:
        from config import NVD_API_KEY
        collector = NVDCollector(api_key=NVD_API_KEY)
        print("âœ… NVD API Key detected (Rate limit: 100 req/min)")
    except ImportError:
        collector = NVDCollector()
        print("âš ï¸ No API key found. Using public rate limit (10 req/min). Data collection will be slow.")
    except Exception:
        collector = NVDCollector()
        print("âš ï¸ Error setting up API key. Using public rate limit (10 req/min).")
    
    # Collect data
    df = collector.collect_cves(
        start_date="2018-01-01",
        end_date="2024-12-31",
        target_count=15000,
        keywords=None
    )
    
    # Save raw data
    data_file = Path('data/nvd_data_15k.csv')
    collector.save_to_csv(df, data_file)
    
    print(f"ğŸ“Š Collected {len(df)} CVEs.")
    print("Vulnerability type distribution (Initial Check):")
    print(df['vuln_type'].value_counts())
    
    return df


def main():
    """Main Stage 2 pipeline"""
    setup_logging()
    logger = logging.getLogger(__name__)
    
    print("="*70)
    print(" "*15 + "GLITCHFORGE - STAGE 2")
    print(" "*10 + "ML Model Development Pipeline")
    print("="*70)
    
    data_dir = Path('data')
    data_dir.mkdir(exist_ok=True)
    
    # ========================================
    # STEP 1: Data Collection
    # ========================================
    print("\n" + "="*70)
    print("STEP 1: Collecting NVD Data")
    print("="*70)
    
    nvd_data_file = data_dir / 'nvd_data_15k.csv'
    
    if nvd_data_file.exists():
        print(f"\nFound existing data file: {nvd_data_file}")
        response = input("Use existing data? (y/n): ").lower()
        if response == 'y':
            df_raw = pd.read_csv(nvd_data_file)
            print(f"âœ… Loaded {len(df_raw)} existing CVEs")
        else:
            df_raw = collect_nvd_data()
    else:
        df_raw = collect_nvd_data()
    
    logger.info(f"Raw data shape: {df_raw.shape}")
    
    # ========================================
    # STEP 2: Feature Engineering
    # ========================================
    print("\n" + "="*70)
    print("STEP 2: Feature Engineering")
    print("="*70)
    
    engineer = FeatureEngineer()
    df_engineered = engineer.engineer_features(df_raw)
    
    engineered_file = data_dir / 'features_engineered.csv'
    df_engineered.to_csv(engineered_file, index=False)
    logger.info(f"Engineered features saved to {engineered_file}")
    
    X, y = engineer.select_features(df_engineered)
    
    logger.info(f"Final Dataset Ready (Features: {X.shape}, Labels: {y.shape})")
    
    # ========================================
    # STEP 3: Model Training
    # ========================================
    print("\n" + "="*70)
    print("STEP 3: Model Training")
    print("="*70)
    
    trainer = MLModelTrainer()
    X_train, X_val, X_test, y_train, y_val, y_test = trainer.prepare_data(X, y)
    
    # Normalize features
    X_train_scaled, X_val_scaled = engineer.normalize_features(X_train, X_val)
    X_test_scaled = pd.DataFrame(
        engineer.scaler.transform(X_test),
        columns=X_test.columns,
        index=X_test.index
    )
    
    # ========================================
    # STEP 3A: Random Forest
    # ========================================
    print("\n" + "-"*70)
    print("Training Random Forest Classifier")
    print("-"*70)
    
    rf_metrics = trainer.train_random_forest(
        X_train_scaled, y_train, X_val_scaled, y_val,
        hyperparameter_tune=False
    )
    
    trainer.save_metrics(rf_metrics, 'rf_training_metrics.json')
    
    # ========================================
    # STEP 3B: Neural Network
    # ========================================
    print("\n" + "-"*70)
    print("Training Neural Network Classifier")
    print("-"*70)
    
    # NOTE: The UnicodeEncodeError happens when trainer.train_neural_network
    # calls model.summary() with the default logging function print_fn=self.logger.info.
    # To fix this without editing model_trainer.py, we suppress the summary print 
    # or accept the error as noise (which we can't fix here fully). 
    # Assuming the rest of training works despite the logged error:
    
    nn_metrics = trainer.train_neural_network(
        X_train_scaled, y_train, X_val_scaled, y_val,
        epochs=100, batch_size=32
    )
    
    trainer.save_metrics(nn_metrics, 'nn_training_metrics.json')
    
    # ========================================
    # STEP 4: Test Set Evaluation
    # ========================================
    print("\n" + "="*70)
    print("STEP 4: Test Set Evaluation")
    print("="*70)
    
    test_metrics = trainer.evaluate_on_test(X_test_scaled, y_test)
    trainer.save_metrics(test_metrics, 'test_metrics.json')
    
    # ========================================
    # STEP 5: Final Summary (Streamlined)
    # ========================================
    print("\n" + "="*70)
    print("STAGE 2 COMPLETE - MODEL COMPARISON")
    print("="*70)
    
    rf_test = test_metrics.get('random_forest', {})
    nn_test = test_metrics.get('neural_network', {})
    rf_acc = rf_test.get('accuracy', 0)
    nn_acc = nn_test.get('accuracy', 0)
    target = 0.90
    
    print("\n--- FINAL TEST ACCURACY ---")
    print(f"Random Forest: {rf_acc:.4f}")
    print(f"Neural Network: {nn_acc:.4f}")
    
    print("\n--- COMPARISON ---")
    if rf_acc > nn_acc:
        print(f"ğŸ† Random Forest performed better (+{(rf_acc - nn_acc)*100:.2f}%)")
    elif nn_acc > rf_acc:
        print(f"ğŸ† Neural Network performed better (+{(nn_acc - rf_acc)*100:.2f}%)")
    else:
        print("ğŸ‘ Both models performed equally.")
        
    print("\n--- TARGET ACHIEVEMENT ---")
    print(f"Target: {target*100:.1f}% accuracy")
    print(f"RF: {'âœ… ACHIEVED' if rf_acc >= target else 'âŒ NOT MET'} ({rf_acc*100:.2f}%)")
    print(f"NN: {'âœ… ACHIEVED' if nn_acc >= target else 'âŒ NOT MET'} ({nn_acc*100:.2f}%)")
    
    if rf_acc >= target or nn_acc >= target:
        print("\nğŸ‰ Stage 2 Target Achieved!")
    else:
        print("\nâš ï¸ Target not met. Review feature engineering or hyperparameters.")
    
    print("\n" + "="*70)
    print("READY FOR STAGE 3: XAI Integration (SHAP + LIME)")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()